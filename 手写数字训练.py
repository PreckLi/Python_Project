"""è®­ç»ƒæ‰‹å†™æ•°å­—"""
import os
import tensorflow as tf
#pycharm2019.3ä¹‹å‰ç‰ˆæœ¬è¯†åˆ«ä¸äº†tensorflow.keras,åªæœ‰è¿™æ ·å†™
try:
    import tensorflow.python.keras as keras
except:
    import tensorflow.keras as keras
from tensorflow.keras import layers,optimizers,datasets  #å¯¼å…¥tfå­åº“ç­‰
(x,y),(x_val,y_val)=datasets.mnist.load_data()  #åŠ è½½mnistæ•°æ®é›†ï¼Œç¬¬ä¸€ä¸ªä¸ºè®­ç»ƒé›†ï¼Œç¬¬äºŒä¸ªä¸ºæµ‹è¯•é›†
"""ä» TensorFlow ä¸­åŠ è½½çš„ MNIST æ•°æ®å›¾ç‰‡ï¼Œæ•°å€¼çš„èŒƒå›´ä¸º[0,255]ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­é—´ï¼Œ
ä¸€èˆ¬å¸Œæœ›æ•°æ®çš„èŒƒå›´åœ¨ 0 å‘¨å›´çš„å°èŒƒå›´å†…åˆ†å¸ƒã€‚é€šè¿‡é¢„å¤„ç†æ­¥éª¤ï¼Œæˆ‘ä»¬æŠŠ[0,255]åƒç´ èŒƒå›´
å½’ä¸€åŒ–(Normalize)åˆ°[0,1.]åŒºé—´ï¼Œå†ç¼©æ”¾åˆ°[âˆ’1,1]åŒºé—´ï¼Œä»è€Œæœ‰åˆ©äºæ¨¡å‹çš„è®­ç»ƒã€‚"""
x=2*tf.convert_to_tensor(x,dtype=tf.float32)/255.-1 #è½¬æ¢ä¸ºæµ®ç‚¹å¼ é‡,å¹¶ç¼©æ”¾åˆ°-1~1
y=tf.convert_to_tensor(y,dtype=tf.int32)    #è½¬æ¢ä¸ºæ•´å‹å¼ é‡
# """å¯ä»¥å°†è¾“å‡ºè®¾ç½®ä¸ºğ‘‘outä¸ªè¾“å‡ºèŠ‚ç‚¹çš„å‘é‡ï¼Œğ‘‘outä¸ç±»åˆ«æ•°ç›¸
# åŒï¼Œè®©ç¬¬ğ‘– âˆˆ [1, ğ‘‘out]ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„å€¼è¡¨ç¤ºå½“å‰æ ·æœ¬å±äºç±»åˆ«ğ‘–çš„æ¦‚ç‡ğ‘ƒ(ğ’™å±äºç±»åˆ«ğ‘–|ğ’™)ã€‚æˆ‘
# ä»¬åªè€ƒè™‘è¾“å…¥å›¾ç‰‡åªè¾“å…¥ä¸€ä¸ªç±»åˆ«çš„æƒ…å†µï¼Œæ­¤æ—¶è¾“å…¥å›¾ç‰‡çš„çœŸå®æ ‡ç­¾å·²ç»å”¯ä¸€ç¡®å®šï¼šå¦‚æœ
# ç‰©ä½“å±äºç¬¬ğ‘–ç±»çš„è¯ï¼Œé‚£ä¹ˆç´¢å¼•ä¸ºğ‘–çš„ä½ç½®ä¸Šè®¾ç½®ä¸º 1ï¼Œå…¶ä»–ä½ç½®è®¾ç½®ä¸º 0ï¼Œæˆ‘ä»¬æŠŠè¿™ç§ç¼–ç 
# æ–¹å¼å«ä½œ one-hot ç¼–ç (ç‹¬çƒ­ç¼–ç )ã€‚"""
# y=tf.one_hot(y,depth=10)    #one-hotç¼–ç 
# print(x.shape,y.shape)
# print("x:",x)
# train_dataset=tf.data.Dataset.from_tensor_slices((x,y)) #æ„å»ºæ•°æ®é›†å¯¹è±¡
# print('train_dataset:',train_dataset)
# train_dataset=train_dataset.batch(512)  #æ‰¹é‡è®­ç»ƒ
# print(train_dataset)


#ç½‘ç»œæ­å»º
#åˆ›å»ºä¸€å±‚ç½‘ç»œï¼Œè®¾ç½®è¾“å‡ºèŠ‚ç‚¹æ•°ä¸º256ï¼Œæ¿€æ´»å‡½æ•°ç±»å‹ä¸ºReLU
layers.Dense(256,activation='relu')
#åˆ©ç”¨sequentialå®¹å™¨å°è£…3ä¸ªç½‘ç»œå±‚,å‰ç½‘ç»œå±‚çš„è¾“å‡ºé»˜è®¤ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥
model=keras.Sequential([    #ä¸‰ä¸ªéçº¿æ€§å±‚çš„åµŒå¥—æ¨¡å‹
    layers.Dense(256,activation='relu'),    #éšè—å±‚1
    layers.Dense(128,activation='relu'),    #éšè—å±‚2
    layers.Dense(10)    #è¾“å‡ºå±‚ï¼Œè¾“å‡ºèŠ‚ç‚¹æ•°ä¸º10
])

#æ¨¡å‹è®­ç»ƒ
with tf.GradientTape() as tape:     #æ„å»ºæ¢¯åº¦è®°å½•ç¯å¢ƒ
    #æ‰“å¹³æ“ä½œ,[b,28,28] =>  [b,784]
    x=tf.reshape(x,(-1,28*28))
    # step1.å¾—åˆ°æ¨¡å‹è¾“å‡ºoutput[b,784]=>[b,10]
    out=model(x)
    #[b]=>[b,10]
    y_onehot=tf.one_hot(y,depth=10)
    #è®¡ç®—å·®çš„å¹³æ–¹å’Œ,[b,10]
    loss=tf.square(out-y_onehot)
    print(loss)
    #è®¡ç®—æ¯ä¸ªæ ·æœ¬å¹³å‡è¯¯å·®,[b]
    loss=tf.reduce_sum(loss)/x.shape[0]
    grads=tape.gradient(loss,model.trainable_variables)
    optimizer = optimizers.SGD(learning_rate=0.001)
    optimizer.apply_gradients(zip(grads,model.trainable_variables))
